head(iris.new)
# Applying the K-means clustering algorithm with no. of centroids(k)=3
# ---
#
result<- kmeans(iris.new,3)
# Previewing the no. of records in each cluster
#
result$size
# Getting the value of cluster center datapoint value(3 centers for k=3)
# ---
#
result$centers
# Getting the cluster vector that shows the cluster where each record falls
# ---
#
result$cluster
# The graph shows that we have got 3 clearly distinguishable clusters for Ozone and Solar.R data points.
# Letâs see how clustering has performed on Wind and Temp attributes.
# Visualizing the  clustering results
# ---
#
par(mfrow = c(1,2), mar = c(5,4,2,2))
# Plotting to see how Ozone and Solar.R data points have been distributed in clusters
# ---
#
plot(airquality[,1:2], col = result$cluster)
## Example
# ---
# Question: Perform clustering analysis on the following dataset using the K-Means clustering algorithm.
# ---
# OUR CODE GOES BELOW
#
# Load and view the dataset
# ---
# Importing the dataset
# ---
#
require("datasets")
# Loading the Iris dataset
# ---
#
data("iris")
# Viewing the structure of the dataset
# ---
#
str(iris)
# Viewing the statistical summary of the dataset
# ---
#
summary(iris)
# Previewing the dataset
# ---
#
head(iris)
# Preprocessing the dataset
# ---
# Since clustering is a type of Unsupervised Learning,
# we would not require Class Label(output) during execution of our algorithm.
# We will, therefore, remove Class Attribute âSpeciesâ and store it in another variable.
# We would then normalize the attributes between 0 and 1 using our own function.
# ---
#
iris.new<- iris[, c(1, 2, 3, 4)]
iris.class<- iris[, "Species"]
head(iris.new)
# Previewing the class column
# ---
#
head(iris.class)
# Normalizing the dataset so that no particular attribute
# has more impact on clustering algorithm than others.
# ---
#
normalize <- function(x){
return ((x-min(x)) / (max(x)-min(x)))
}
iris.new$Sepal.Length<- normalize(iris.new$Sepal.Length)
iris.new$Sepal.Width<- normalize(iris.new$Sepal.Width)
iris.new$Petal.Length<- normalize(iris.new$Petal.Length)
iris.new$Petal.Width<- normalize(iris.new$Petal.Width)
head(iris.new)
# Applying the K-means clustering algorithm with no. of centroids(k)=3
# ---
#
result<- kmeans(iris.new,3)
# Previewing the no. of records in each cluster
#
result$size
# Getting the value of cluster center datapoint value(3 centers for k=3)
# ---
#
result$centers
# Getting the cluster vector that shows the cluster where each record falls
# ---
#
result$cluster
# The graph shows that we have got 3 clearly distinguishable clusters for Ozone and Solar.R data points.
# Letâs see how clustering has performed on Wind and Temp attributes.
# Visualizing the  clustering results
# ---
#
par(mfrow = c(1,2), mar = c(5,4,2,2))
# Plotting to see how Ozone and Solar.R data points have been distributed in clusters
# ---
#
plot(airquality[,1:2], col = result$cluster)
# Verifying the results of clustering
# ---
#
par(mfrow = c(2,2), mar = c(5,4,2,2))
# Plotting to see how Sepal.Length and Sepal.Width data points have been distributed in clusters
plot(iris.new[c(1,2)], col = result$cluster)
# Plotting to see how Sepal.Length and Sepal.Width data points have been distributed
# originally as per "class" attribute in dataset
# ---
#
plot(iris.new[c(1,2)], col = iris.class)
# Plotting to see how Petal.Length and Petal.Width data points have been distributed in clusters
# ---
#
plot(iris.new[c(3,4)], col = result$cluster)
plot(iris.new[c(3,4)], col = iris.class)
# Result of table shows that Cluster 1 corresponds to Virginica,
# Cluster 2 corresponds to Versicolor and Cluster 3 to Setosa.
# ---
#
table(result$cluster, iris.class)
## Challenge 1
# ---
# Question: Apply unsupervised learning to the given airquality dataset below.
# ---
# OUR CODE GOES BELOW
#
# Load and view the dataset
# ---
# Importing the dataset
# ---
#
data("airquality")
str(airquality)
## Challenge 2
# ---
# Question: Create a model that clusters the following dataset.
# ---
# Dataset = http://bit.ly/SalaryDatasetClustering
# ---
# OUR CODE GOES BELOW
#
## Challenge 3
# ---
# Question: Cluster customers from the given wholesale customer database.
# ---
# Dataset source = https://archive.ics.uci.edu/ml/datasets/Wholesale+customers
# ---
# OUR CODE GOES BELOW
#
yg821jf8611_ny_albany_2020_04_01 <- readRDS("C:/Users/Admin/Downloads/yg821jf8611_ny_albany_2020_04_01.rds")
install.packages(c("backports", "BH", "brio", "cpp11", "crosstalk", "DBI", "diffobj", "fansi", "ggrepel", "hms", "htmltools", "pROC", "quantreg", "Rcpp", "SQUAREM", "tibble", "tinytex", "withr"))
# Loading our transactions dataset from our csv file
# ---
# We will use read.transactions fuction which will load data from comma-separated files
# and convert them to the class transactions, which is the kind of data that
# we will require while working with models of association rules
# ---
#
path <-"http://bit.ly/GroceriesDataset"
head(read.csv(path))
Transactions<-read.transactions(path)
# Loading the arules library
#
library(arules)
# Loading our transactions dataset from our csv file
# ---
# We will use read.transactions fuction which will load data from comma-separated files
# and convert them to the class transactions, which is the kind of data that
# we will require while working with models of association rules
# ---
#
path <-"http://bit.ly/GroceriesDataset"
head(read.csv(path))
Transactions<-read.transactions(path)
Transactions
# Loading our transactions dataset from our csv file
# ---
# We will use read.transactions fuction which will load data from comma-separated files
# and convert them to the class transactions, which is the kind of data that
# we will require while working with models of association rules
# ---
#
path <-"http://bit.ly/GroceriesDataset"
head(read.csv(path))
Transactions<-read.transactions(path, sep = ",")
Transactions
path <- "http://bit.ly/SupermarketDatasetII"
transactions<-read.transactions(path, sep = ",")
transactions
library(arules)
path <- "http://bit.ly/SupermarketDatasetII"
transactions<-read.transactions(path, sep = ",")
transactions
path <- "http://bit.ly/SupermarketDatasetII"
transactions<-read.transactions(path, sep = ",")
transactions
duplicated_rows <- transactions[duplicated(transactions),]
nrow(duplicated_rows)
path <- "http://bit.ly/SupermarketDatasetII"
transactions<-read.transactions(path, sep = ",")
head(transactions)
duplicated_rows <- transactions[duplicated(transactions),]
nrow(duplicated_rows)
transactions <- transactions[!duplicated(transactions), ]
duplicated_rows <- transactions[duplicated(transactions),]
nrow(duplicated_rows)
path <- "http://bit.ly/SupermarketDatasetII"
transactions<-read.transactions(path, sep = ",")
transactions
duplicated_rows <- transactions[duplicated(transactions),]
nrow(duplicated_rows)
transactions <- transactions[!duplicated(transactions), ]
duplicated_rows <- transactions[duplicated(transactions),]
nrow(duplicated_rows)
class(transactions)
inspect(Transactions[1:5])
inspect(transactions[1:5])
summary(Transactions)
summary(transactions)
# We first we install the required arules library
#
install.packages("arules")
# Loading the arules library
#
library(arules)
# Loading our transactions dataset from our csv file
# ---
# We will use read.transactions fuction which will load data from comma-separated files
# and convert them to the class transactions, which is the kind of data that
# we will require while working with models of association rules
# ---
#
path <-"http://bit.ly/GroceriesDataset"
head(read.csv(path))
install.packages("arules")
Transactions<-read.transactions(path, sep = ",")
Transactions
# Verifying the object's class
# ---
# This should show us transactions as the type of data that we will need
# ---
#
class(Transactions)
# Previewing our first 5 transactions
#
inspect(Transactions[1:5])
# If we wanted to preview the items that make up our dataset,
# alternatively we can do the following
# ---
#
items<-as.data.frame(itemLabels(Transactions))
colnames(items) <- "Item"
head(items, 19)
# Generating a summary of the transaction dataset
# ---
# This would give us some information such as the most purchased items,
# distribution of the item sets (no. of items purchased in each transaction), etc.
# ---
#
summary(Transactions)
# Exploring the frequency of some articles
# i.e. transacations ranging from 8 to 10 and performing
# some operation in percentage terms of the total transactions
#
itemFrequency(Transactions[, 8:10],type = "absolute")
round(itemFrequency(Transactions[, 8:10],type = "relative")*100,2)
# Producing a chart of frequencies and fitering
# to consider only items with a minimum percentage
# of support/ considering a top x of items
# ---
# Displaying top 10 most common items in the transactions dataset
# and the items whose relative importance is at least 10%
#
par(mfrow = c(1, 2))
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col="darkgreen")
itemFrequencyPlot(Transactions, support = 0.1, col="darkred")
# Building a model based on association rules
# using the apriori function
# ---
# We use Min Support as 0.001 and confidence as 0.8
# ---
#
rules <- apriori (Transactions, parameter = list(supp = 0.001, conf = 0.8))
rules
# We use measures of significance and interest on the rules,
# determining which ones are interesting and which to discard.
# ---
# However since we built the model using 0.001 Min support
# and confidence as 0.8 we obtained 410 rules.
# However, in order to illustrate the sensitivity of the model to these two parameters,
# we will see what happens if we increase the support or lower the confidence level
#
# Building a apriori model with Min Support as 0.002 and confidence as 0.8.
rules2 <- apriori (Transactions,parameter = list(supp = 0.002, conf = 0.8))
# Building apriori model with Min Support as 0.002 and confidence as 0.6.
rules3 <- apriori (Transactions, parameter = list(supp = 0.001, conf = 0.6))
rules2
rules3
# We can perform an exploration of our model
# through the use of the summary function as shown
# ---
# Upon running the code, the function would give us information about the model
# i.e. the size of rules, depending on the items that contain these rules.
# In our above case, most rules have 3 and 4 items though some rules do have upto 6.
# More statistical information such as support, lift and confidence is also provided.
# ---
#
summary(rules)
# Observing rules built in our model i.e. first 5 model rules
# ---
#
inspect(rules[1:5])
# Interpretation of the first rule:
# ---
# If someone buys liquor and red/blush wine, they are 90% likely to buy bottled beer too
# ---
# Ordering these rules by a criteria such as the level of confidence
# then looking at the first five rules.
# We can also use different criteria such as: (by = "lift" or by = "support")
#
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
# Interpretation
# ---
# The given five rules have a confidence of 100
# ---
# If we're interested in making a promotion relating to the sale of yogurt,
# we could create a subset of rules concerning these products
# ---
# This would tell us the items that the customers bought before purchasing yogurt
# ---
#
yogurt <- subset(rules, subset = rhs %pin% "yogurt")
# Then order by confidence
yogurt<-sort(yogurt, by="confidence", decreasing=TRUE)
inspect(yogurt[1:5])
# What if we wanted to determine items that customers might buy
# who have previously bought yogurt?
# ---
#
# Subset the rules
yogurt <- subset(rules, subset = lhs %pin% "yogurt")
# Order by confidence
yogurt<-sort(yogurt, by="confidence", decreasing=TRUE)
# inspect top 5
inspect(yogurt[15:19])
## Challenge 1
# ---
# Question: Build an apriori model previewing the rules with the highest confidence interval
# given the following interval.
# ---
# Dataset url = http://bit.ly/AssociativeAnalysisDataset
# ---
# OUR CODE GOES BELOW
#
## Challenge 2
# ---
# Question:
# ---
# Question: Build an apriori model previewing the rules with the highest confidence interval.
# given the following interval.
# ---
# OUR CODE GOES BELOW
#
library("colorspace")
pal <- choose_palette()
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col=pal)
pal <- choose_palette()
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col=pal)
pal <- choose_palette()
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col=pal)
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col="darkgreen")
itemFrequencyPlot(Transactions, support = 0.1, col="darkred")
# Producing a chart of frequencies and fitering
# to consider only items with a minimum percentage
# of support/ considering a top x of items
# ---
# Displaying top 10 most common items in the transactions dataset
# and the items whose relative importance is at least 10%
#
par(mfrow = c(1, 2))
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col="darkgreen")
itemFrequencyPlot(Transactions, support = 0.1, col="darkred")
par(mfrow = c(1, 2))
# plot the frequency of items
itemFrequencyPlot(Transactions, topN = 10, col="darkgreen")
par(mfrow = c(1, 2))
# plot the frequency of items
itemFrequencyPlot(transactions, topN = 10, col="darkgreen")
rules <- apriori (Transactions, parameter = list(supp = 0.001, conf = 0.8))
rules
rules <- apriori (transactions, parameter = list(supp = 0.001, conf = 0.8))
rules
summary(rules)
knitr::opts_chunk$set(collapse=TRUE, comment="##", fig.retina=2, fig.path = "README_figs/README-")
data <- read.csv("http://bit.ly/CarreFourDataset")
head(data)
data = subset(data, select = -c(Invoice.ID) )
head(data)
# install.packages("magrittr") # package installations are only needed the first time you use it
# install.packages("dplyr")    # alternative installation of the %>%
library(magrittr) # needs to be run every time you start R and want to use %>%
library(dplyr)    # alternatively, this also loads %>%
data_2 <- data %>% select(Total, gross.income, gross.margin.percentage, cogs, Unit.price, Quantity)
data_2
# install.packages("reprex")
library(reprex)
data_2 <- data_2[ - as.numeric(which(apply(data_2, 2, var) == 0))]
data_2.pca <- prcomp(data_2[,c(1:ncol(data_2))], center = TRUE, scale. = TRUE)
summary(data_2.pca)
str(data_2.pca)
library(devtools)
# install.packages("usethis")
# install.packages("ggbiplot")
library(usethis)
library(ggbiplot)
ggbiplot(data_2.pca)
ggbiplot(data_2.pca, labels=data$Gender, obs.scale = 1, var.scale = 1)
ggbiplot(data_2.pca, labels=data$, ellipse=TRUE, groups = data$Customer.type, obs.scale = 1, var.scale = 1)
ggbiplot(data_2.pca, ellipse=TRUE, groups = data$Customer.type, obs.scale = 1, var.scale = 1)
ggbiplot(data_2.pca, ellipse=TRUE, groups = data$Product.line, obs.scale = 1, var.scale = 1)
inspect(rules[1:5])
rules<-sort(rules, by="confidence", decreasing=TRUE)
inspect(rules[1:5])
# Subset the rules
spaghetti <- subset(rules, subset = lhs %pin% "spaghetti")
# Order by confidence
# Subset the rules
spaghetti <- subset(rules, subset = lhs %pin% "spaghetti")
# Order by confidence
spaghetti<-sort(spaghetti, by="confidence", decreasing=TRUE)
# inspect top 5
inspect(spaghetti[15:19])
security_access_logs <- read_csv(logs_path) %>%
group_by(server) %>%
as_tbl_time(date)
security_access_logs <- read_csv(logs_path) %>%
group_by(date) %>%
as_tbl_time(date)
library(tibbletime)
security_access_logs <- data %>%
group_by(date) %>%
as_tbl_time(date)
data %>%
group_by(date) %>%
as_tbl_time(date)
data <- read.csv("http://bit.ly/CarreFourSalesDataset")
head(data)
duplicated_rows <- data[duplicated(data),]
nrow(duplicated_rows)
data <- data[!duplicated(data), ]
duplicated_rows <- data[duplicated(data),]
nrow(duplicated_rows)
colSums(is.na(data))
library(tibbletime)
data %>%
group_by(date) %>%
as_tbl_time(date)
data %>%
group_by(date) %>%
as_tbl_time(date)
data %>%
group_by(Date) %>%
as_tbl_time(Date)
data %>%
as_tbl_time(Date)
data <- read.csv("http://bit.ly/CarreFourSalesDataset")
head(data)
data %>%
as_tbl_time(Date)
data %>%
as_tbl_time(Date)
head(data)
head(data)
data$Date <- as.date(data$Date)
data$Date <- mdy(data$Date)
library(lubridate, warn.conflicts = FALSE)
data$Date <- mdy(data$Date)
library(lubridate, warn.conflicts = FALSE)
data$Date <- mdy(data$Date)
library(lubridate)
data$Date <- mdy(data$Date)
library(lubridate)
data$Date <- mdy(data$Date)
head(data)
library(tidyverse)
library(lubridate)
data$Date <- mdy(data$Date)
head(data)
library(lubridate)
data$Date <- mdy(data$Date)
head(data)
library(lubridate, warn.conflicts = FALSE)
data$Date <- mdy(data$Date)
library(lubridate, warn.conflicts = FALSE)
data$Date <- mdy(data$Date)
head(data)
install.packages("devtools")
devtools::install_github("tidyverse/lubridate")
